    # gradio interface
    theme = gr.Theme.load("funclip/utils/theme.json")
    with gr.Blocks(theme=theme) as funclip_service:
        with gr.Row():
            with gr.Column():
                with gr.Tab("ğŸ¥ è§†é¢‘/éŸ³é¢‘è¯†åˆ« | Video/Audio Recognition"):
                    video_input = gr.Video(label="ğŸ“¹ è§†é¢‘æ–‡ä»¶ | Video File")
                    audio_input = gr.Audio(label="ğŸ§ éŸ³é¢‘æ–‡ä»¶ | Audio File")
                    hotwords_input = gr.Textbox(label="ğŸ”¥ çƒ­è¯ | Hotwords (å¤šä¸ªçƒ­è¯ç”¨é€—å·åˆ†éš”)")
                    output_dir = gr.Textbox(label="ğŸ“‚ è¾“å‡ºç›®å½• | Output Directory (å¯é€‰)")
                    with gr.Row():
                        recog_button = gr.Button("ğŸ™ï¸ è¯†åˆ« | Recognize", variant="primary")
                        recog_button2 = gr.Button("ğŸ™ï¸ è¯†åˆ«+è¯´è¯äºº | Recognize+Speaker")
                    video_text_output = gr.Textbox(label="ğŸ“ è¯†åˆ«æ–‡æœ¬ | Recognized Text")
                    video_srt_output = gr.Textbox(label="ğŸ“– è¯†åˆ«SRTå­—å¹• | Recognized SRT Subtitles")
                with gr.Tab("ğŸ§  LLMæ™ºèƒ½è£å‰ª | LLM AI Clipping"):
                    with gr.Row():
                        prompt_head = gr.Textbox(label="Prompt Systemï¼ˆä¸éœ€è¦ä¿®æ”¹ï¼Œä¼šè‡ªåŠ¨æ‹¼æ¥å·¦ä¸‹è§’çš„srtå­—å¹•ï¼‰", value=("è¿™æ˜¯å¾…è£å‰ªçš„è§†é¢‘srtå­—å¹•ï¼š"))
                    with gr.Column():
                        with gr.Row():
                            llm_model = gr.Dropdown(
                                choices=["qwen-plus", "gpt-3.5-turbo", "gpt-3.5-turbo-0125", "gpt-4-turbo", "g4f-gpt-3.5-turbo", "moonshot-v1-8K"],
                                value="qwen-plus",
                                label="LLM Model Name"
                            )
                            apikey_input = gr.Textbox(label="APIKEY")
                            api_base_input = gr.Textbox(label="APIåœ°å€(å¯é€‰)", placeholder="https://api.openai.com/v1")
                            clip_count = gr.Number(label="ç”Ÿæˆç‰‡æ®µæ•°é‡ | Clip Count", value=1, precision=0, minimum=1)
                            clip_second = gr.Number(label="æ¯ä¸ªç‰‡æ®µæ—¶é•¿ | Clip Second", value=180, precision=0, minimum=1)
                        llm_button = gr.Button("LLMæ¨ç† | LLM Inferenceï¼ˆé¦–å…ˆè¿›è¡Œè¯†åˆ«ï¼Œég4féœ€é…ç½®å¯¹åº”apikeyï¼‰", variant="primary")
                    llm_result = gr.Textbox(label="LLM Clipper Result")
                    with gr.Row():
                        llm_clip_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª | AI Clip", variant="primary")
                        llm_clip_subti_button = gr.Button("ğŸ§  LLMæ™ºèƒ½è£å‰ª+å­—å¹• | AI Clip+Subtitles")
                with gr.Tab("âœ‚ï¸ æ ¹æ®æ–‡æœ¬/è¯´è¯äººè£å‰ª | Text/Speaker Clipping"):
                    video_text_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªæ–‡æœ¬ | Text to Clip (å¤šæ®µæ–‡æœ¬ä½¿ç”¨'#'è¿æ¥)")
                    video_spk_input = gr.Textbox(label="âœï¸ å¾…è£å‰ªè¯´è¯äºº | Speaker to Clip (å¤šä¸ªè¯´è¯äººä½¿ç”¨'#'è¿æ¥)")
                    with gr.Row():
                        clip_button = gr.Button("âœ‚ï¸ è£å‰ª | Clip", variant="primary")
                        clip_subti_button = gr.Button("âœ‚ï¸ è£å‰ª+å­—å¹• | Clip+Subtitles")
                    with gr.Row():
                        video_start_ost = gr.Slider(minimum=-500, maximum=1000, value=0, step=50, label="âª å¼€å§‹ä½ç½®åç§» | Start Offset (ms)")
                        video_end_ost = gr.Slider(minimum=-500, maximum=1000, value=100, step=50, label="â© ç»“æŸä½ç½®åç§» | End Offset (ms)")
                with gr.Row():
                    font_size = gr.Slider(minimum=10, maximum=100, value=32, step=2, label="ğŸ”  å­—å¹•å­—ä½“å¤§å° | Subtitle Font Size")
                    font_color = gr.Radio(["black", "white", "green", "red"], label="ğŸŒˆ å­—å¹•é¢œè‰² | Subtitle Color", value='white')
            video_output = gr.Video(label="è£å‰ªç»“æœ | Video Clipped")
            audio_output = gr.Audio(label="è£å‰ªç»“æœ | Audio Clipped")
            clip_message = gr.Textbox(label="âš ï¸ è£å‰ªä¿¡æ¯ | Clipping Log")
            srt_clipped = gr.Textbox(label="ğŸ“– è£å‰ªéƒ¨åˆ†SRTå­—å¹•å†…å®¹ | Clipped RST Subtitles")
        
        recog_button.click(mix_recog, inputs=[video_input, audio_input, hotwords_input, output_dir], outputs=[video_text_output, video_srt_output, video_state, audio_state])
        recog_button2.click(mix_recog_speaker, inputs=[video_input, audio_input, hotwords_input, output_dir], outputs=[video_text_output, video_srt_output, video_state, audio_state])
        clip_button.click(mix_clip, inputs=[video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir], outputs=[video_output, audio_output, clip_message, srt_clipped])
        clip_subti_button.click(video_clip_addsub, inputs=[video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, output_dir, font_size, font_color], outputs=[video_output, clip_message, srt_clipped])
        llm_button.click(llm_inference, inputs=[prompt_head, prompt_head2, video_srt_output, llm_model, apikey_input, api_base_input, clip_count, clip_second], outputs=[llm_result])
        llm_clip_button.click(AI_clip, inputs=[llm_result, video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir, clip_count], outputs=[video_output, audio_output, clip_message, srt_clipped])
        llm_clip_subti_button.click(AI_clip_subti, inputs=[llm_result, video_text_input, video_spk_input, video_start_ost, video_end_ost, video_state, audio_state, output_dir], outputs=[video_output, audio_output, clip_message, srt_clipped])
    
    # start gradio service in local or share
    if args.listen:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name, inbrowser=False)
    else:
        funclip_service.launch(share=args.share, server_port=args.port, server_name=server_name)